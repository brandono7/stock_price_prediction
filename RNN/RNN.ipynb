{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84f15710-dff1-42f7-bc2e-8278d4286243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb6aa4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "TRAIN_DATA, TRAIN_LABELS = None, None\n",
    "VAL_DATA, VAL_LABELS = None, None\n",
    "TEST_DATA, TEST_LABELS = None, None\n",
    "DATA_SCALER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a27e48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "combined_stocks_df = pd.read_csv(\"filtered_stocks_combined.csv\")\n",
    "\n",
    "# Convert Date to datetime and set as index\n",
    "combined_stocks_df[\"Date\"] = pd.to_datetime(combined_stocks_df[\"Date\"])\n",
    "combined_stocks_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Drop unnecessary columns if any\n",
    "combined_stocks_df = combined_stocks_df.drop(columns=[\"index\"])  # Optional\n",
    "\n",
    "# Pivot to multi-level columns: Ticker as level 1, feature as level 2\n",
    "stocks_df = combined_stocks_df.pivot_table(\n",
    "    index=combined_stocks_df.index,\n",
    "    columns=\"ticker\",\n",
    "    values=[col for col in combined_stocks_df.columns if col != \"ticker\"]\n",
    ")\n",
    "\n",
    "# Sort columns for clarity\n",
    "stocks_df = stocks_df.sort_index(axis=1, level=0)\n",
    "\n",
    "# Swap the column MultiIndex levels\n",
    "stocks_df_leveled = stocks_df.swaplevel(axis=1)\n",
    "\n",
    "# Sort by ticker (Level 0)\n",
    "stocks_df_leveled = stocks_df_leveled.sort_index(axis=1, level=0)\n",
    "\n",
    "#Time-based split into 60% train, 20% val, 20% test\n",
    "train_dict, val_dict, test_dict = {}, {}, {}\n",
    "\n",
    "# Time-based split into 60% train, 20% val, 20% test\n",
    "train_dict, val_dict, test_dict = {}, {}, {}\n",
    "\n",
    "# Compute Logarithmic Returns, split into train, val and test sets\n",
    "for ticker in stocks_df_leveled.columns.levels[0]:\n",
    "    stocks_df_leveled.loc[:, (ticker, 'log_return')] = np.log(\n",
    "        stocks_df_leveled[ticker]['Close'] / stocks_df_leveled[ticker]['Close'].shift(1)\n",
    "    )\n",
    "\n",
    "    df = stocks_df_leveled[ticker].dropna().sort_index()\n",
    "    total_len = len(df)\n",
    "    train_end = int(total_len * 0.6)\n",
    "    val_end = train_end + int(total_len * 0.2)\n",
    "\n",
    "    train_dict[ticker] = df.iloc[:train_end]\n",
    "    val_dict[ticker] = df.iloc[train_end:val_end]\n",
    "    test_dict[ticker] = df.iloc[val_end:]\n",
    "\n",
    "train_df = pd.concat(train_dict, names=[\"Ticker\", \"Date\"])\n",
    "val_df = pd.concat(val_dict, names=[\"Ticker\", \"Date\"])\n",
    "test_df = pd.concat(test_dict, names=[\"Ticker\", \"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240789ee-6642-4d1c-9e0b-ee221af5024e",
   "metadata": {},
   "source": [
    "## Normalization/Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e99e7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\", \"log_return\"]\n",
    "target = [\"log_return\"]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "train_df[features] = scaler.fit_transform(train_df[features])\n",
    "val_df[features] = scaler.transform(val_df[features])\n",
    "test_df[features] = scaler.transform(test_df[features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26286f5d-de44-4749-918b-d61545dc6b9b",
   "metadata": {},
   "source": [
    "## Creating Sequences\n",
    "Define sequences of past 2 weeks (10 days), or the look back window to predict the price on a rolling window basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b68b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for LSTM input\n",
    "def create_sequences(df, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        X.append(df.iloc[i:i+seq_length, :-1].values)  # Features (excluding target)\n",
    "        y.append(1 if df.iloc[i+seq_length, -1] > 0 else 0)  # Label: 1 (Long) or 0 (Short)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 10\n",
    "X_train, y_train = create_sequences(train_df[features + target], seq_length)\n",
    "X_val, y_val = create_sequences(val_df[features + target], seq_length)\n",
    "X_test, y_test = create_sequences(test_df[features + target], seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0331713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54abc0",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c3cbaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, \n",
    "                        activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Example usage \n",
    "# demo_model = create_RNN(2, 1, (3,1), activation=['linear', 'linear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2061f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and evaluate RNN models\n",
    "def train_evaluate_rnn(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    hidden_units=3, \n",
    "    dense_units=1, \n",
    "    seq_length=None,\n",
    "    activation=['tanh', 'tanh'],\n",
    "    epochs=5,\n",
    "    batch_size=1,\n",
    "    verbose=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate an RNN model with the given parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train: Training data\n",
    "    X_val, y_val: Validation data\n",
    "    X_test, y_test: Test data\n",
    "    hidden_units: Number of units in RNN layer\n",
    "    dense_units: Number of units in output dense layer\n",
    "    seq_length: Length of input sequences (will be inferred if None)\n",
    "    activation: Activation functions for RNN and dense layers\n",
    "    epochs: Maximum number of training epochs\n",
    "    batch_size: Batch size for training\n",
    "    verbose: Verbosity level for training (0=silent, 1=progress bar, 2=one line per epoch)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict containing model, history, and evaluation metrics\n",
    "    \"\"\"\n",
    "    # If seq_length not provided, infer from data\n",
    "    if seq_length is None:\n",
    "        seq_length = X_train.shape[1]\n",
    "    \n",
    "    # Create model\n",
    "    input_shape = (seq_length, X_train.shape[2] if len(X_train.shape) > 2 else 1)\n",
    "    model = create_RNN(hidden_units=hidden_units, dense_units=dense_units, \n",
    "                       input_shape=input_shape, activation=activation)\n",
    "    \n",
    "    # Train model without early stopping\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Return all relevant information\n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'metrics': {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        },\n",
    "        'predictions': y_pred,\n",
    "        'parameters': {\n",
    "            'hidden_units': hidden_units,\n",
    "            'dense_units': dense_units,\n",
    "            'activation': activation,\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a35a09",
   "metadata": {},
   "outputs": [],
   "source": [
    " Call the train_evaluate_rnn function\n",
    "results = train_evaluate_rnn(\n",
    "    X_train=X_train, \n",
    "    y_train=y_train,\n",
    "    X_val=X_val, \n",
    "    y_val=y_val,\n",
    "    X_test=X_test, \n",
    "    y_test=y_test,\n",
    "    hidden_units=3,          # Adjust as needed\n",
    "    dense_units=1,\n",
    "    seq_length=None,         # Will be inferred from data\n",
    "    activation=['tanh', 'tanh'],\n",
    "    epochs=10,               # Adjust as needed\n",
    "    batch_size=1,            # Adjust as needed\n",
    "    verbose=1                # 1 for progress bar\n",
    ")\n",
    "\n",
    "# Now results contains everything: model, history, metrics, etc.\n",
    "# You can access individual components:\n",
    "\n",
    "# Get the trained model\n",
    "model = results['model']\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"MSE: {results['metrics']['mse']:.4f}\")\n",
    "print(f\"RMSE: {results['metrics']['rmse']:.4f}\")\n",
    "print(f\"MAE: {results['metrics']['mae']:.4f}\")\n",
    "print(f\"R²: {results['metrics']['r2']:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results['history'].history['loss'], label='Training Loss')\n",
    "plt.plot(results['history'].history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(results['predictions'], label='Predicted')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54bb9af4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 6 and 1 for '{{node sequential_1/simple_rnn_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1/simple_rnn_1/strided_slice_2, sequential_1/simple_rnn_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [?,6], [1,3].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(None, 6), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 3), dtype=float32)',)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\djleong01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\djleong01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 6 and 1 for '{{node sequential_1/simple_rnn_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1/simple_rnn_1/strided_slice_2, sequential_1/simple_rnn_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [?,6], [1,3].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(None, 6), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 3), dtype=float32)',)\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = (model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f2492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct_predictions \u001b[38;5;241m/\u001b[39m total_samples\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Compute and print accuracy\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m final_accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy(test_df, \u001b[43my_pred\u001b[49m, seq_length)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Model Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate model accuracy based on actual next day Open & Close prices\n",
    "def calculate_accuracy(test_df, y_pred, seq_length):\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(y_pred)\n",
    "\n",
    "    for i in range(seq_length, len(test_df) - 1):\n",
    "        actual_position = 1 if test_df.iloc[i + 1]['Close'] > test_df.iloc[i + 1]['Open'] else 0\n",
    "        predicted_position = y_pred[i - seq_length]\n",
    "        if actual_position == predicted_position:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return correct_predictions / total_samples\n",
    "\n",
    "# Compute and print accuracy\n",
    "final_accuracy = calculate_accuracy(test_df, y_pred, seq_length)\n",
    "print(f\"Final Model Accuracy: {final_accuracy:.2f}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
