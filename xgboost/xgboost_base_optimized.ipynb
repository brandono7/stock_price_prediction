{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, f1_score, \n",
    "                            confusion_matrix, roc_auc_score,\n",
    "                            RocCurveDisplay)\n",
    "\n",
    "train_df = pd.read_csv(\"./datasets/train.csv\")\n",
    "val_df = pd.read_csv(\"./datasets/val.csv\")\n",
    "test_df = pd.read_csv(\"./datasets/test.csv\")\n",
    "\n",
    "# Define features and binary classification target\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "train_df['target'] = (train_df['log_return'] > 0).astype(int)\n",
    "val_df['target'] = (val_df['log_return'] > 0).astype(int)\n",
    "test_df['target'] = (test_df['log_return'] > 0).astype(int)\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_val = val_df[features]\n",
    "y_val = val_df['target']\n",
    "\n",
    "X_test = test_df[features]\n",
    "y_test = test_df['target']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "RandomSearch best params: {'subsample': 1.0, 'reg_lambda': 20, 'reg_alpha': 0, 'n_estimators': 1200, 'min_child_weight': 7, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "RandomSearch best CV acc: 0.7008\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n"
     ]
    }
   ],
   "source": [
    "# ─── 2. Base GPU‑powered XGBClassifier ───────────────────────────────────────\n",
    "base_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    # use_label_encoder=False,\n",
    "    verbosity=0,\n",
    "    random_state=42,\n",
    "    tree_method='gpu_hist',       # GPU training\n",
    "    predictor='gpu_predictor',    # GPU prediction\n",
    "    gpu_id=0                      # which GPU to use\n",
    ")\n",
    "\n",
    "# ─── 3. RandomizedSearchCV over broad ranges ────────────────────────────────\n",
    "param_dist = {\n",
    "    'n_estimators':     [100, 300, 500, 800, 1200],\n",
    "    'learning_rate':    [0.01, 0.03, 0.05, 0.1, 0.2],\n",
    "    'max_depth':        [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma':            [0, 0.1, 0.5, 1, 2],\n",
    "    'subsample':        [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha':        [0, 1, 5, 10],\n",
    "    'reg_lambda':       [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=base_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rand_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"RandomSearch best params:\", rand_search.best_params_)\n",
    "print(\"RandomSearch best CV acc: {:.4f}\".format(rand_search.best_score_))\n",
    "\n",
    "# ─── 4. GridSearchCV fine‑tuning around Randomized best ─────────────────────\n",
    "best = rand_search.best_params_\n",
    "grid_params = {\n",
    "    'max_depth':        [max(1, best['max_depth']-1), best['max_depth'], best['max_depth']+1],\n",
    "    'min_child_weight': [max(1, best['min_child_weight']-1), best['min_child_weight'], best['min_child_weight']+1],\n",
    "    'gamma':            [max(0, best['gamma']-0.5), best['gamma'], best['gamma']+0.5],\n",
    "    'subsample':        [best['subsample']-0.1, best['subsample'], best['subsample']+0.1],\n",
    "    'colsample_bytree': [best['colsample_bytree']-0.1, best['colsample_bytree'], best['colsample_bytree']+0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        **{k: best[k] for k in ['n_estimators','learning_rate','reg_alpha','reg_lambda']},\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        # use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        gpu_id=0\n",
    "    ),\n",
    "    param_grid=grid_params,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"GridSearch best params:\", grid_search.best_params_)\n",
    "print(\"GridSearch best CV acc: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "# ─── 5. Final fit with early stopping (GPU) ─────────────────────────────────\n",
    "# Extend n_estimators so early stopping can trim it back\n",
    "final_params = {\n",
    "    **grid_search.best_params_,\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'reg_alpha':     best['reg_alpha'],\n",
    "    'reg_lambda':    best['reg_lambda'],\n",
    "    'tree_method':   'gpu_hist',\n",
    "    'predictor':     'gpu_predictor',\n",
    "    'gpu_id':        0\n",
    "}\n",
    "\n",
    "final_clf = xgb.XGBClassifier(\n",
    "    **final_params,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    # use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "final_clf.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    # early_stopping_rounds=30,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ─── 6. Evaluate on test ────────────────────────────────────────────────────\n",
    "y_test_pred = final_clf.predict(X_test_scaled)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(\"\\nFinal Test Accuracy (GPU): {:.4f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs3244",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
