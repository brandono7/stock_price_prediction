{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f15710-dff1-42f7-bc2e-8278d4286243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e4354a-ecd4-47dd-bc39-4c9da925dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stocks_df = pd.read_csv(\"filtered_stocks_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc84ae1-a909-46e2-9e6e-c9a951f0463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>index</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>OpenInt</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>45725</td>\n",
       "      <td>41.600</td>\n",
       "      <td>42.293</td>\n",
       "      <td>41.600</td>\n",
       "      <td>42.204</td>\n",
       "      <td>123930383</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>45726</td>\n",
       "      <td>42.579</td>\n",
       "      <td>42.579</td>\n",
       "      <td>42.023</td>\n",
       "      <td>42.426</td>\n",
       "      <td>86135637</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>45727</td>\n",
       "      <td>42.260</td>\n",
       "      <td>42.817</td>\n",
       "      <td>42.196</td>\n",
       "      <td>42.772</td>\n",
       "      <td>70669988</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>45728</td>\n",
       "      <td>42.902</td>\n",
       "      <td>42.932</td>\n",
       "      <td>42.632</td>\n",
       "      <td>42.739</td>\n",
       "      <td>83619699</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>45729</td>\n",
       "      <td>42.787</td>\n",
       "      <td>43.073</td>\n",
       "      <td>42.503</td>\n",
       "      <td>43.045</td>\n",
       "      <td>86506108</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75495</th>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>14745947</td>\n",
       "      <td>87.556</td>\n",
       "      <td>87.625</td>\n",
       "      <td>87.201</td>\n",
       "      <td>87.383</td>\n",
       "      <td>4428429</td>\n",
       "      <td>0</td>\n",
       "      <td>XOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75496</th>\n",
       "      <td>2016-12-27</td>\n",
       "      <td>14745948</td>\n",
       "      <td>87.499</td>\n",
       "      <td>87.768</td>\n",
       "      <td>87.257</td>\n",
       "      <td>87.423</td>\n",
       "      <td>5100402</td>\n",
       "      <td>0</td>\n",
       "      <td>XOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75497</th>\n",
       "      <td>2016-12-28</td>\n",
       "      <td>14745949</td>\n",
       "      <td>87.354</td>\n",
       "      <td>87.768</td>\n",
       "      <td>86.949</td>\n",
       "      <td>86.989</td>\n",
       "      <td>6834213</td>\n",
       "      <td>0</td>\n",
       "      <td>XOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75498</th>\n",
       "      <td>2016-12-29</td>\n",
       "      <td>14745950</td>\n",
       "      <td>86.797</td>\n",
       "      <td>87.277</td>\n",
       "      <td>86.728</td>\n",
       "      <td>87.036</td>\n",
       "      <td>6938299</td>\n",
       "      <td>0</td>\n",
       "      <td>XOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75499</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>14745951</td>\n",
       "      <td>86.728</td>\n",
       "      <td>87.373</td>\n",
       "      <td>86.710</td>\n",
       "      <td>86.949</td>\n",
       "      <td>9464343</td>\n",
       "      <td>0</td>\n",
       "      <td>XOM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date     index    Open    High     Low   Close     Volume  \\\n",
       "0      2011-01-03     45725  41.600  42.293  41.600  42.204  123930383   \n",
       "1      2011-01-04     45726  42.579  42.579  42.023  42.426   86135637   \n",
       "2      2011-01-05     45727  42.260  42.817  42.196  42.772   70669988   \n",
       "3      2011-01-06     45728  42.902  42.932  42.632  42.739   83619699   \n",
       "4      2011-01-07     45729  42.787  43.073  42.503  43.045   86506108   \n",
       "...           ...       ...     ...     ...     ...     ...        ...   \n",
       "75495  2016-12-23  14745947  87.556  87.625  87.201  87.383    4428429   \n",
       "75496  2016-12-27  14745948  87.499  87.768  87.257  87.423    5100402   \n",
       "75497  2016-12-28  14745949  87.354  87.768  86.949  86.989    6834213   \n",
       "75498  2016-12-29  14745950  86.797  87.277  86.728  87.036    6938299   \n",
       "75499  2016-12-30  14745951  86.728  87.373  86.710  86.949    9464343   \n",
       "\n",
       "       OpenInt ticker  \n",
       "0            0   AAPL  \n",
       "1            0   AAPL  \n",
       "2            0   AAPL  \n",
       "3            0   AAPL  \n",
       "4            0   AAPL  \n",
       "...        ...    ...  \n",
       "75495        0    XOM  \n",
       "75496        0    XOM  \n",
       "75497        0    XOM  \n",
       "75498        0    XOM  \n",
       "75499        0    XOM  \n",
       "\n",
       "[75500 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_stocks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a565a32-c860-4323-8c73-1dda0cedc269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th colspan=\"6\" halign=\"left\">AAPL</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ABT</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">WMT</th>\n",
       "      <th colspan=\"6\" halign=\"left\">XOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>OpenInt</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>...</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>OpenInt</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>OpenInt</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-03</th>\n",
       "      <td>42.204</td>\n",
       "      <td>42.293</td>\n",
       "      <td>41.600</td>\n",
       "      <td>41.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123930383.0</td>\n",
       "      <td>19.358</td>\n",
       "      <td>19.549</td>\n",
       "      <td>19.338</td>\n",
       "      <td>19.549</td>\n",
       "      <td>...</td>\n",
       "      <td>46.027</td>\n",
       "      <td>46.089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16789275.0</td>\n",
       "      <td>60.341</td>\n",
       "      <td>60.535</td>\n",
       "      <td>59.604</td>\n",
       "      <td>59.668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28807445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>42.426</td>\n",
       "      <td>42.579</td>\n",
       "      <td>42.023</td>\n",
       "      <td>42.579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86135637.0</td>\n",
       "      <td>19.540</td>\n",
       "      <td>19.561</td>\n",
       "      <td>19.350</td>\n",
       "      <td>19.448</td>\n",
       "      <td>...</td>\n",
       "      <td>46.139</td>\n",
       "      <td>46.419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14296931.0</td>\n",
       "      <td>60.625</td>\n",
       "      <td>60.665</td>\n",
       "      <td>60.235</td>\n",
       "      <td>60.469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24744869.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>42.772</td>\n",
       "      <td>42.817</td>\n",
       "      <td>42.196</td>\n",
       "      <td>42.260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70669988.0</td>\n",
       "      <td>19.540</td>\n",
       "      <td>19.764</td>\n",
       "      <td>19.460</td>\n",
       "      <td>19.520</td>\n",
       "      <td>...</td>\n",
       "      <td>46.174</td>\n",
       "      <td>46.539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16723328.0</td>\n",
       "      <td>60.462</td>\n",
       "      <td>60.600</td>\n",
       "      <td>60.009</td>\n",
       "      <td>60.430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20448359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>42.739</td>\n",
       "      <td>42.932</td>\n",
       "      <td>42.632</td>\n",
       "      <td>42.902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83619699.0</td>\n",
       "      <td>19.500</td>\n",
       "      <td>19.682</td>\n",
       "      <td>19.342</td>\n",
       "      <td>19.634</td>\n",
       "      <td>...</td>\n",
       "      <td>45.712</td>\n",
       "      <td>46.166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18335156.0</td>\n",
       "      <td>60.853</td>\n",
       "      <td>61.052</td>\n",
       "      <td>60.439</td>\n",
       "      <td>60.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27829692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>43.045</td>\n",
       "      <td>43.073</td>\n",
       "      <td>42.503</td>\n",
       "      <td>42.787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86506108.0</td>\n",
       "      <td>19.582</td>\n",
       "      <td>19.615</td>\n",
       "      <td>19.444</td>\n",
       "      <td>19.489</td>\n",
       "      <td>...</td>\n",
       "      <td>45.698</td>\n",
       "      <td>45.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9374462.0</td>\n",
       "      <td>61.182</td>\n",
       "      <td>61.431</td>\n",
       "      <td>60.777</td>\n",
       "      <td>60.876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23838996.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker        AAPL                                                  ABT  \\\n",
       "             Close    High     Low    Open OpenInt       Volume   Close   \n",
       "Date                                                                      \n",
       "2011-01-03  42.204  42.293  41.600  41.600     0.0  123930383.0  19.358   \n",
       "2011-01-04  42.426  42.579  42.023  42.579     0.0   86135637.0  19.540   \n",
       "2011-01-05  42.772  42.817  42.196  42.260     0.0   70669988.0  19.540   \n",
       "2011-01-06  42.739  42.932  42.632  42.902     0.0   83619699.0  19.500   \n",
       "2011-01-07  43.045  43.073  42.503  42.787     0.0   86506108.0  19.582   \n",
       "\n",
       "ticker                              ...     WMT                              \\\n",
       "              High     Low    Open  ...     Low    Open OpenInt      Volume   \n",
       "Date                                ...                                       \n",
       "2011-01-03  19.549  19.338  19.549  ...  46.027  46.089     0.0  16789275.0   \n",
       "2011-01-04  19.561  19.350  19.448  ...  46.139  46.419     0.0  14296931.0   \n",
       "2011-01-05  19.764  19.460  19.520  ...  46.174  46.539     0.0  16723328.0   \n",
       "2011-01-06  19.682  19.342  19.634  ...  45.712  46.166     0.0  18335156.0   \n",
       "2011-01-07  19.615  19.444  19.489  ...  45.698  45.810     0.0   9374462.0   \n",
       "\n",
       "ticker         XOM                                              \n",
       "             Close    High     Low    Open OpenInt      Volume  \n",
       "Date                                                            \n",
       "2011-01-03  60.341  60.535  59.604  59.668     0.0  28807445.0  \n",
       "2011-01-04  60.625  60.665  60.235  60.469     0.0  24744869.0  \n",
       "2011-01-05  60.462  60.600  60.009  60.430     0.0  20448359.0  \n",
       "2011-01-06  60.853  61.052  60.439  60.625     0.0  27829692.0  \n",
       "2011-01-07  61.182  61.431  60.777  60.876     0.0  23838996.0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Date to datetime and set as index\n",
    "combined_stocks_df[\"Date\"] = pd.to_datetime(combined_stocks_df[\"Date\"])\n",
    "combined_stocks_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Drop unnecessary columns if any\n",
    "combined_stocks_df = combined_stocks_df.drop(columns=[\"index\"])  # Optional\n",
    "\n",
    "# Pivot to multi-level columns: Ticker as level 1, feature as level 2\n",
    "stocks_df = combined_stocks_df.pivot_table(\n",
    "    index=combined_stocks_df.index,\n",
    "    columns=\"ticker\",\n",
    "    values=[col for col in combined_stocks_df.columns if col != \"ticker\"]\n",
    ")\n",
    "\n",
    "# Sort columns for clarity\n",
    "stocks_df = stocks_df.sort_index(axis=1, level=0)\n",
    "\n",
    "\n",
    "# Swap the column MultiIndex levels\n",
    "stocks_df_leveled = stocks_df.swaplevel(axis=1)\n",
    "\n",
    "# Sort by ticker (Level 0)\n",
    "stocks_df_leveled = stocks_df_leveled.sort_index(axis=1, level=0)\n",
    "\n",
    "# Preview the new structure\n",
    "stocks_df_leveled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04be6c37-d14a-4e1b-91f0-69ddc8c2bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Time-based split into 60% train, 20% val, 20% test\n",
    "train_dict, val_dict, test_dict = {}, {}, {}\n",
    "\n",
    "for ticker in stocks_df_leveled.columns.levels[0]:\n",
    "    stocks_df_leveled.loc[:, (ticker, 'log_return')] = np.log(\n",
    "    stocks_df_leveled[ticker]['Close'] / stocks_df_leveled[ticker]['Close'].shift(1)\n",
    ")\n",
    "\n",
    "    df = stocks_df_leveled[ticker].dropna().sort_index()\n",
    "    total_len = len(df)\n",
    "    train_end = int(total_len * 0.6)\n",
    "    val_end = train_end + int(total_len * 0.2)\n",
    "\n",
    "    train_dict[ticker] = df.iloc[:train_end]\n",
    "    val_dict[ticker] = df.iloc[train_end:val_end]\n",
    "    test_dict[ticker] = df.iloc[val_end:]\n",
    "\n",
    "train_df = pd.concat(train_dict, names=[\"Ticker\", \"Date\"])\n",
    "val_df = pd.concat(val_dict, names=[\"Ticker\", \"Date\"])\n",
    "test_df = pd.concat(test_dict, names=[\"Ticker\", \"Date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a80427f-5677-4acb-9b58-137e910b4f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>OpenInt</th>\n",
       "      <th>Volume</th>\n",
       "      <th>log_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AAPL</th>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>42.426</td>\n",
       "      <td>42.579</td>\n",
       "      <td>42.023</td>\n",
       "      <td>42.579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86135637.0</td>\n",
       "      <td>0.005246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>42.772</td>\n",
       "      <td>42.817</td>\n",
       "      <td>42.196</td>\n",
       "      <td>42.260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70669988.0</td>\n",
       "      <td>0.008122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>42.739</td>\n",
       "      <td>42.932</td>\n",
       "      <td>42.632</td>\n",
       "      <td>42.902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83619699.0</td>\n",
       "      <td>-0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>43.045</td>\n",
       "      <td>43.073</td>\n",
       "      <td>42.503</td>\n",
       "      <td>42.787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86506108.0</td>\n",
       "      <td>0.007134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-10</th>\n",
       "      <td>43.855</td>\n",
       "      <td>43.956</td>\n",
       "      <td>43.179</td>\n",
       "      <td>43.393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124888228.0</td>\n",
       "      <td>0.018643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">XOM</th>\n",
       "      <th>2014-08-04</th>\n",
       "      <td>88.145</td>\n",
       "      <td>88.411</td>\n",
       "      <td>86.542</td>\n",
       "      <td>86.921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13804459.0</td>\n",
       "      <td>0.013374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-05</th>\n",
       "      <td>86.445</td>\n",
       "      <td>87.625</td>\n",
       "      <td>86.077</td>\n",
       "      <td>87.492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14847864.0</td>\n",
       "      <td>-0.019475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-06</th>\n",
       "      <td>87.133</td>\n",
       "      <td>87.723</td>\n",
       "      <td>86.445</td>\n",
       "      <td>86.445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11264688.0</td>\n",
       "      <td>0.007927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-07</th>\n",
       "      <td>86.507</td>\n",
       "      <td>87.889</td>\n",
       "      <td>85.944</td>\n",
       "      <td>87.828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11379351.0</td>\n",
       "      <td>-0.007210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-08</th>\n",
       "      <td>87.800</td>\n",
       "      <td>87.847</td>\n",
       "      <td>86.551</td>\n",
       "      <td>86.947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12730751.0</td>\n",
       "      <td>0.014836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45250 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Close    High     Low    Open  OpenInt       Volume  \\\n",
       "Ticker Date                                                               \n",
       "AAPL   2011-01-04  42.426  42.579  42.023  42.579      0.0   86135637.0   \n",
       "       2011-01-05  42.772  42.817  42.196  42.260      0.0   70669988.0   \n",
       "       2011-01-06  42.739  42.932  42.632  42.902      0.0   83619699.0   \n",
       "       2011-01-07  43.045  43.073  42.503  42.787      0.0   86506108.0   \n",
       "       2011-01-10  43.855  43.956  43.179  43.393      0.0  124888228.0   \n",
       "...                   ...     ...     ...     ...      ...          ...   \n",
       "XOM    2014-08-04  88.145  88.411  86.542  86.921      0.0   13804459.0   \n",
       "       2014-08-05  86.445  87.625  86.077  87.492      0.0   14847864.0   \n",
       "       2014-08-06  87.133  87.723  86.445  86.445      0.0   11264688.0   \n",
       "       2014-08-07  86.507  87.889  85.944  87.828      0.0   11379351.0   \n",
       "       2014-08-08  87.800  87.847  86.551  86.947      0.0   12730751.0   \n",
       "\n",
       "                   log_return  \n",
       "Ticker Date                    \n",
       "AAPL   2011-01-04    0.005246  \n",
       "       2011-01-05    0.008122  \n",
       "       2011-01-06   -0.000772  \n",
       "       2011-01-07    0.007134  \n",
       "       2011-01-10    0.018643  \n",
       "...                       ...  \n",
       "XOM    2014-08-04    0.013374  \n",
       "       2014-08-05   -0.019475  \n",
       "       2014-08-06    0.007927  \n",
       "       2014-08-07   -0.007210  \n",
       "       2014-08-08    0.014836  \n",
       "\n",
       "[45250 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1485c81a-df56-429b-8117-c96872111c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 7971.8940 - val_loss: 16212.7705\n",
      "Epoch 2/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8118.3633 - val_loss: 16183.0166\n",
      "Epoch 3/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7890.3804 - val_loss: 16124.9229\n",
      "Epoch 4/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7971.2441 - val_loss: 16017.2490\n",
      "Epoch 5/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7909.8320 - val_loss: 15840.0723\n",
      "Epoch 6/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7580.3950 - val_loss: 15581.1104\n",
      "Epoch 7/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7383.7891 - val_loss: 15234.1221\n",
      "Epoch 8/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7265.6162 - val_loss: 14802.7578\n",
      "Epoch 9/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6976.6162 - val_loss: 14299.6924\n",
      "Epoch 10/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6724.1636 - val_loss: 13740.1191\n",
      "Epoch 11/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6166.3818 - val_loss: 13144.0771\n",
      "Epoch 12/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5667.4321 - val_loss: 12531.3926\n",
      "Epoch 13/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5431.9170 - val_loss: 11911.1641\n",
      "Epoch 14/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5058.1460 - val_loss: 11305.6416\n",
      "Epoch 15/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4765.9321 - val_loss: 10725.2305\n",
      "Epoch 16/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4650.9624 - val_loss: 10177.1221\n",
      "Epoch 17/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4189.3301 - val_loss: 9669.3740\n",
      "Epoch 18/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3917.8589 - val_loss: 9197.1172\n",
      "Epoch 19/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3599.6240 - val_loss: 8769.3682\n",
      "Epoch 20/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3375.7563 - val_loss: 8363.8379\n",
      "Epoch 21/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3143.7432 - val_loss: 7993.5737\n",
      "Epoch 22/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2939.5471 - val_loss: 7658.3604\n",
      "Epoch 23/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2898.6399 - val_loss: 7345.7959\n",
      "Epoch 24/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2602.1262 - val_loss: 7049.2935\n",
      "Epoch 25/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2479.1389 - val_loss: 6778.3286\n",
      "Epoch 26/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2341.1040 - val_loss: 6512.7095\n",
      "Epoch 27/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2242.8069 - val_loss: 6264.6978\n",
      "Epoch 28/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2067.1191 - val_loss: 6037.1045\n",
      "Epoch 29/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1930.5563 - val_loss: 5812.5669\n",
      "Epoch 30/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1815.5901 - val_loss: 5593.3970\n",
      "Epoch 31/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1703.2054 - val_loss: 5387.0557\n",
      "Epoch 32/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1598.1210 - val_loss: 5194.6294\n",
      "Epoch 33/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1542.3242 - val_loss: 5002.1294\n",
      "Epoch 34/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1440.8241 - val_loss: 4832.4995\n",
      "Epoch 35/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1338.5923 - val_loss: 4641.3335\n",
      "Epoch 36/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1319.1886 - val_loss: 4469.7720\n",
      "Epoch 37/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1174.5251 - val_loss: 4317.7485\n",
      "Epoch 38/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1127.8682 - val_loss: 4171.7432\n",
      "Epoch 39/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1063.8416 - val_loss: 4025.7534\n",
      "Epoch 40/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 970.5731 - val_loss: 3888.8599\n",
      "Epoch 41/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 937.5455 - val_loss: 3753.9890\n",
      "Epoch 42/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 905.7554 - val_loss: 3626.1255\n",
      "Epoch 43/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 807.7486 - val_loss: 3507.4248\n",
      "Epoch 44/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 790.5734 - val_loss: 3399.1672\n",
      "Epoch 45/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 758.8882 - val_loss: 3296.3765\n",
      "Epoch 46/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 725.6072 - val_loss: 3195.7053\n",
      "Epoch 47/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 652.5331 - val_loss: 3104.3125\n",
      "Epoch 48/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 639.9493 - val_loss: 3015.2144\n",
      "Epoch 49/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 638.5364 - val_loss: 2930.9868\n",
      "Epoch 50/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 613.8824 - val_loss: 2858.8420\n",
      "Epoch 51/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 605.3130 - val_loss: 2781.5371\n",
      "Epoch 52/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 578.7181 - val_loss: 2718.9146\n",
      "Epoch 53/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 552.1665 - val_loss: 2668.4795\n",
      "Epoch 54/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 547.6546 - val_loss: 2609.4761\n",
      "Epoch 55/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 530.3958 - val_loss: 2541.5073\n",
      "Epoch 56/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 538.7748 - val_loss: 2510.5791\n",
      "Epoch 57/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 515.1705 - val_loss: 2465.9258\n",
      "Epoch 58/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 475.5103 - val_loss: 2411.6306\n",
      "Epoch 59/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 493.9756 - val_loss: 2365.5269\n",
      "Epoch 60/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 490.0219 - val_loss: 2338.2388\n",
      "Epoch 61/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 459.8823 - val_loss: 2312.7434\n",
      "Epoch 62/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 446.9944 - val_loss: 2288.8831\n",
      "Epoch 63/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 455.3001 - val_loss: 2269.7632\n",
      "Epoch 64/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 457.2130 - val_loss: 2250.5930\n",
      "Epoch 65/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 455.8057 - val_loss: 2234.7012\n",
      "Epoch 66/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 443.8429 - val_loss: 2222.6975\n",
      "Epoch 67/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 437.4649 - val_loss: 2206.7507\n",
      "Epoch 68/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 445.0000 - val_loss: 2183.8977\n",
      "Epoch 69/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 429.5142 - val_loss: 2160.1309\n",
      "Epoch 70/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 448.8708 - val_loss: 2155.1255\n",
      "Epoch 71/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 448.6619 - val_loss: 2140.4336\n",
      "Epoch 72/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 457.5729 - val_loss: 2114.9033\n",
      "Epoch 73/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 458.5558 - val_loss: 2123.1511\n",
      "Epoch 74/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 436.1127 - val_loss: 2117.1597\n",
      "Epoch 75/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 442.5923 - val_loss: 2107.1296\n",
      "Epoch 76/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 459.5476 - val_loss: 2091.9709\n",
      "Epoch 77/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 454.2574 - val_loss: 2105.5613\n",
      "Epoch 78/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 469.5431 - val_loss: 2080.9993\n",
      "Epoch 79/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 456.1591 - val_loss: 2084.8186\n",
      "Epoch 80/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 442.0568 - val_loss: 2087.8064\n",
      "Epoch 81/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 445.8084 - val_loss: 2094.6899\n",
      "Epoch 82/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 434.9085 - val_loss: 2067.1265\n",
      "Epoch 83/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 460.9897 - val_loss: 2072.2180\n",
      "Epoch 84/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 457.0355 - val_loss: 2060.5828\n",
      "Epoch 85/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 440.3843 - val_loss: 2072.3081\n",
      "Epoch 86/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 424.7349 - val_loss: 2058.6750\n",
      "Epoch 87/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 440.5873 - val_loss: 2056.4949\n",
      "Epoch 88/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 439.3872 - val_loss: 2061.6084\n",
      "Epoch 89/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 453.7905 - val_loss: 2034.7330\n",
      "Epoch 90/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 457.5668 - val_loss: 2046.7928\n",
      "Epoch 91/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 445.6500 - val_loss: 2053.2917\n",
      "Epoch 92/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 435.6738 - val_loss: 2060.0986\n",
      "Epoch 93/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 427.7487 - val_loss: 2066.6648\n",
      "Epoch 94/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 424.2558 - val_loss: 2043.7166\n",
      "Epoch 95/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 438.9591 - val_loss: 2049.3257\n",
      "Epoch 96/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 431.1470 - val_loss: 2050.0591\n",
      "Epoch 97/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 438.0140 - val_loss: 2065.9248\n",
      "Epoch 98/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 439.2200 - val_loss: 2038.6267\n",
      "Epoch 99/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 423.0122 - val_loss: 2065.6509\n",
      "Epoch 100/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 444.6885 - val_loss: 2061.4966\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Test Mean Squared Error (MSE): 7958.5865\n",
      "Test Mean Absolute Error (MAE): 41.9830\n",
      "Directional Accuracy for Close Prices: 0.8973\n"
     ]
    }
   ],
   "source": [
    "# Function to create rolling window sequences\n",
    "def create_rolling_window_data(df, window_size=10, target_shift=1):\n",
    "    X, y = [], []\n",
    "    tickers = df.columns.levels[0]  # Level 0 is tickers after swaplevel\n",
    "    num_tickers = len(tickers)\n",
    "    \n",
    "    for i in range(window_size, len(df) - target_shift):\n",
    "        window = df.iloc[i - window_size:i]\n",
    "        X.append(window.values.flatten())\n",
    "        \n",
    "        next_day = df.iloc[i + target_shift - 1]\n",
    "        targets = []\n",
    "        for ticker in tickers:\n",
    "            targets.append(next_day[(ticker, 'Open')])\n",
    "            targets.append(next_day[(ticker, 'Close')])\n",
    "        y.append(targets)\n",
    "    \n",
    "    return np.array(X), np.array(y), num_tickers\n",
    "\n",
    "# Create sequences from the preprocessed DataFrame\n",
    "X, y, num_tickers = create_rolling_window_data(stocks_df_leveled)\n",
    "\n",
    "# Split data into train, validation, and test sets (60%, 20%, 20%)\n",
    "total_size = len(X)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the MLP model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_tickers * 2)  # Output layer: number of tickers * 2 (open and close)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "# Calculate directional accuracy for close prices\n",
    "actual_close = y_test[:, 1::2]  # Close prices (every second element)\n",
    "predicted_close = y_pred[:, 1::2]\n",
    "features = ['Open', 'Close', 'High', 'Low', 'Volume']\n",
    "close_idx = features.index('Close')  # 1\n",
    "num_features = len(features)  # 5\n",
    "window_size = 10\n",
    "positions = [(window_size - 1) * num_tickers * num_features + i * num_features + close_idx for i in range(num_tickers)]\n",
    "last_close = X_test[:, positions]\n",
    "\n",
    "actual_change = actual_close - last_close\n",
    "predicted_change = predicted_close - last_close\n",
    "directional_accuracy = np.mean(np.sign(actual_change) == np.sign(predicted_change))\n",
    "\n",
    "print(f\"Directional Accuracy for Close Prices: {directional_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "240789ee-6642-4d1c-9e0b-ee221af5024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 5: Validation MPE = 24.3605%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 6: Validation MPE = 25.4227%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 7: Validation MPE = 24.4748%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 8: Validation MPE = 25.6162%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 9: Validation MPE = 25.2775%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 10: Validation MPE = 25.5835%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 11: Validation MPE = 24.7604%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 12: Validation MPE = 25.0203%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 13: Validation MPE = 25.2813%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 14: Validation MPE = 25.2146%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 15: Validation MPE = 24.3716%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 16: Validation MPE = 25.1255%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 17: Validation MPE = 24.8159%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 18: Validation MPE = 25.2200%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 19: Validation MPE = 24.7027%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size 20: Validation MPE = 24.6962%\n",
      "Best Window Size: 5 with Validation MPE: 24.3605%\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gao23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 6173.6865 - val_loss: 16180.3271\n",
      "Epoch 2/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7760.9883 - val_loss: 16158.6445\n",
      "Epoch 3/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7966.4126 - val_loss: 16135.2588\n",
      "Epoch 4/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7821.6406 - val_loss: 16108.7266\n",
      "Epoch 5/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7789.2051 - val_loss: 16076.6318\n",
      "Epoch 6/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7881.1211 - val_loss: 16036.3887\n",
      "Epoch 7/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7868.3691 - val_loss: 15985.8799\n",
      "Epoch 8/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7704.4951 - val_loss: 15922.5547\n",
      "Epoch 9/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7707.7935 - val_loss: 15843.2852\n",
      "Epoch 10/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7730.7793 - val_loss: 15745.3301\n",
      "Epoch 11/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7492.2769 - val_loss: 15626.2432\n",
      "Epoch 12/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7445.3906 - val_loss: 15480.7002\n",
      "Epoch 13/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7801.5654 - val_loss: 15308.6504\n",
      "Epoch 14/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7265.8394 - val_loss: 15106.5215\n",
      "Epoch 15/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7234.7681 - val_loss: 14872.5020\n",
      "Epoch 16/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7139.6123 - val_loss: 14606.2432\n",
      "Epoch 17/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6845.7236 - val_loss: 14307.5195\n",
      "Epoch 18/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6577.5186 - val_loss: 13978.9062\n",
      "Epoch 19/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6429.0190 - val_loss: 13619.4971\n",
      "Epoch 20/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6100.5190 - val_loss: 13235.2305\n",
      "Epoch 21/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5909.8936 - val_loss: 12827.4092\n",
      "Epoch 22/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5633.7773 - val_loss: 12399.9961\n",
      "Epoch 23/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5376.6460 - val_loss: 11963.5996\n",
      "Epoch 24/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5018.7793 - val_loss: 11522.9785\n",
      "Epoch 25/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4735.3311 - val_loss: 11078.5254\n",
      "Epoch 26/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4459.5708 - val_loss: 10633.8301\n",
      "Epoch 27/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4490.4238 - val_loss: 10198.2939\n",
      "Epoch 28/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4132.9614 - val_loss: 9778.4951\n",
      "Epoch 29/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3771.2004 - val_loss: 9371.3877\n",
      "Epoch 30/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3473.1558 - val_loss: 8982.5596\n",
      "Epoch 31/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3337.4949 - val_loss: 8605.0684\n",
      "Epoch 32/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3052.7695 - val_loss: 8247.4912\n",
      "Epoch 33/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3216.4407 - val_loss: 7905.3418\n",
      "Epoch 34/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2976.4026 - val_loss: 7586.0918\n",
      "Epoch 35/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2568.9128 - val_loss: 7293.2549\n",
      "Epoch 36/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2464.3914 - val_loss: 7006.6558\n",
      "Epoch 37/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2456.2632 - val_loss: 6732.8960\n",
      "Epoch 38/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2262.7354 - val_loss: 6476.6851\n",
      "Epoch 39/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2152.2026 - val_loss: 6236.5630\n",
      "Epoch 40/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2007.5449 - val_loss: 6006.5244\n",
      "Epoch 41/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1961.3073 - val_loss: 5792.3931\n",
      "Epoch 42/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1890.2142 - val_loss: 5585.7583\n",
      "Epoch 43/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1729.9158 - val_loss: 5387.8716\n",
      "Epoch 44/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1631.5114 - val_loss: 5199.8091\n",
      "Epoch 45/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1486.7035 - val_loss: 5023.3374\n",
      "Epoch 46/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1425.8386 - val_loss: 4852.2227\n",
      "Epoch 47/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1303.6052 - val_loss: 4688.5215\n",
      "Epoch 48/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1248.5148 - val_loss: 4526.7358\n",
      "Epoch 49/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1300.1859 - val_loss: 4367.6313\n",
      "Epoch 50/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1178.7521 - val_loss: 4229.9805\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Test Mean Percentage Error (MPE): 29.7258%\n",
      "Directional Accuracy for Close Prices: 0.9433\n"
     ]
    }
   ],
   "source": [
    "# Function to create rolling window sequences (unchanged)\n",
    "def create_rolling_window_data(df, window_size=10, target_shift=1):\n",
    "    X, y = [], []\n",
    "    tickers = df.columns.levels[0]\n",
    "    num_tickers = len(tickers)\n",
    "    \n",
    "    for i in range(window_size, len(df) - target_shift):\n",
    "        window = df.iloc[i - window_size:i]\n",
    "        X.append(window.values.flatten())\n",
    "        next_day = df.iloc[i + target_shift - 1]\n",
    "        targets = []\n",
    "        for ticker in tickers:\n",
    "            targets.append(next_day[(ticker, 'Open')])\n",
    "            targets.append(next_day[(ticker, 'Close')])\n",
    "        y.append(targets)\n",
    "    \n",
    "    return np.array(X), np.array(y), num_tickers\n",
    "\n",
    "# Function to train and evaluate a model for a given window size\n",
    "def evaluate_window_size(df, window_size):\n",
    "    X, y, num_tickers = create_rolling_window_data(df, window_size=window_size)\n",
    "    \n",
    "    # Split data (60% train, 20% val, 20% test)\n",
    "    total_size = len(X)\n",
    "    train_size = int(0.6 * total_size)\n",
    "    val_size = int(0.2 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "    X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Build and train model\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(num_tickers * 2)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), \n",
    "              epochs=50, batch_size=32, verbose=0)  # Silent training\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = model.predict(X_val_scaled, verbose=0)\n",
    "    val_mpe = np.mean((y_val - y_val_pred) / (y_val + 1e-10)) * 100\n",
    "    return val_mpe, X, y, num_tickers\n",
    "\n",
    "# Test window sizes and find the best\n",
    "window_sizes = range(5, 21)  # Test 5 to 20 days\n",
    "mpe_scores = {}\n",
    "for window_size in window_sizes:\n",
    "    val_mpe, X, y, num_tickers = evaluate_window_size(stocks_df_leveled, window_size)\n",
    "    mpe_scores[window_size] = val_mpe\n",
    "    print(f\"Window Size {window_size}: Validation MPE = {val_mpe:.4f}%\")\n",
    "\n",
    "best_window_size = min(mpe_scores, key=mpe_scores.get)  # Lowest MPE (least negative or smallest positive)\n",
    "print(f\"Best Window Size: {best_window_size} with Validation MPE: {mpe_scores[best_window_size]:.4f}%\")\n",
    "\n",
    "# Final model with best window size\n",
    "X, y, num_tickers = create_rolling_window_data(stocks_df_leveled, window_size=best_window_size)\n",
    "total_size = len(X)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_tickers * 2)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate with MPE\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mpe = np.mean((y_test - y_pred) / (y_test + 1e-10)) * 100\n",
    "print(f\"Test Mean Percentage Error (MPE): {mpe:.4f}%\")\n",
    "\n",
    "# Directional accuracy\n",
    "actual_close = y_test[:, 1::2]\n",
    "predicted_close = y_pred[:, 1::2]\n",
    "features = ['Open', 'Close', 'High', 'Low', 'Volume']\n",
    "close_idx = features.index('Close')\n",
    "num_features = len(features)\n",
    "positions = [(best_window_size - 1) * num_tickers * num_features + i * num_features + close_idx for i in range(num_tickers)]\n",
    "last_close = X_test[:, positions]\n",
    "actual_change = actual_close - last_close\n",
    "predicted_change = predicted_close - last_close\n",
    "directional_accuracy = np.mean(np.sign(actual_change) == np.sign(predicted_change))\n",
    "print(f\"Directional Accuracy for Close Prices: {directional_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26286f5d-de44-4749-918b-d61545dc6b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92625a-00f2-4c49-a595-d2ea0f1b7b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c310db-2dd4-4936-8084-a06e65af136d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
